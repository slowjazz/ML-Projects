# Deep learning, RNNs, and Time-series prediction

A lot of us are familiar with the term 'deep learning' nowadays, especially since its recent explosion in industry popularity. Though the practice has yet to come under rigorous scrutiny in academia, companies are loving its far-reaching applications to a variety of problems. From image recognition and language processing, deep learning also has a future in the field of reinforcement learning. Problems in reinforcement learning are well-illustrated by the OpenAIGym platform (LINK). Reinforcement learning is centered on the idea of making optimal decisions in an environment, given state observations and rewards from previous actions. To better understand this relationship, we look into a type of deep learning model: Recurrent Neural Networks (RNNs). 

## RNNs: Just another neural net?

Neural networks are machine learning models structured after the neurons in our brains. Essentially, neural nets continually learn to define a mathematical function from a set of inputs and outputs. **Recurrent Neural Networks** build on this model by feeding neuron outputs as inputs in a sort of loop. This setup allows neurons to operate on past information like memory. Another characteristic called long short-term memory (LSTM) 

RNNs have become particularly useful for natural language processing because of their ability to perceive context. For example, consider the sentence *In France I loved to speak ___*. An ML model trained with some vocabulary could infer that the next word should be the name of a language. However, without the RNN architecture


-intro to neural nets, RNNs
--applications of RNNs
--applications of RNNs to time-series

-problem of project

-other cool problems 



ideas:
openAIGYM, other reinforcement learning stuff 

add links to read more about:
-deep learning
-RNN
-https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82 
-



Kenneth Li